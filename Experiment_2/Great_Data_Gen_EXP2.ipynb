{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Note: For optimal performance use A100 GPU\n",
        "!pip install synthcity\n",
        "!pip install pycox\n",
        "!pip install be_great"
      ],
      "metadata": {
        "id": "Up2lmbRHE5FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torchaudio torchdata\n",
        "from synthcity.metrics import Metrics\n",
        "from synthcity.plugins.core.dataloader import SurvivalAnalysisDataLoader\n",
        "from synthcity.plugins import Plugins\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import logging\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from pycox import datasets"
      ],
      "metadata": {
        "id": "W75Hs6LZE7zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBpk_RCIvxRP"
      },
      "outputs": [],
      "source": [
        "from be_great import GReaT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data=pd.read_csv('/path/flchain_final.csv')\n",
        "# data=data.drop('Unnamed: 0',axis=1)\n",
        "# data = data[data['duration'] != 0]\n",
        "\n",
        "data = datasets.gbsg.read_df()\n",
        "data = data[data['duration'] != 0]\n",
        "\n",
        "# Use any dataset that you want to use"
      ],
      "metadata": {
        "id": "_7M6jMpLwKq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "wpjGOYIa1ItV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from peft import LoraConfig\n",
        "great = GReaT(\"distilgpt2\",                  # Name of the large language model used (see HuggingFace for more options)\n",
        "              epochs=1000,                   # Number of epochs to train\n",
        "              save_steps=10000,               # Save model weights every x steps\n",
        "              logging_steps=500,             # Log the loss and learning rate every x steps\n",
        "              experiment_dir=\"trainer_iris\", # Name of the directory where all intermediate steps are saved\n",
        "              batch_size=32,                 # Batch Size\n",
        "              # efficient_finetuning='lora'\n",
        "              #lr_scheduler_type=\"constant\", # Specify the learning rate scheduler\n",
        "              #learning_rate=5e-5            # Set the inital learning rate\n",
        "             )"
      ],
      "metadata": {
        "id": "dWMunKxCwKoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = great.fit(data)"
      ],
      "metadata": {
        "id": "wA84N705wKih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "great.save('/path/Great_gbsg_1000')"
      ],
      "metadata": {
        "id": "aR_eg5eA4h8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_hist = trainer.state.log_history.copy()\n",
        "loss_hist.pop()\n",
        "loss = [x[\"loss\"] for x in loss_hist]\n",
        "epochs = [x[\"epoch\"] for x in loss_hist]\n",
        "\n",
        "plt.plot(epochs, loss)"
      ],
      "metadata": {
        "id": "yX-phf-8z_7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate unconditional samples\n",
        "n_samples = len(data)\n",
        "samples = great.sample(n_samples, k=50)"
      ],
      "metadata": {
        "id": "-s0xL6OR0QDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples.head()"
      ],
      "metadata": {
        "id": "L_EpMWm20hTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Benchmark Generate Data\n",
        "\n",
        "loader1 = SurvivalAnalysisDataLoader(data, target_column=\"event\", time_to_event_column=\"duration\")\n",
        "loader2 = SurvivalAnalysisDataLoader(samples, target_column=\"event\", time_to_event_column=\"duration\")\n",
        "\n",
        "met_df = Metrics.evaluate(X_gt=loader1, X_syn=loader2, task_type='survival_analysis', metrics={\n",
        "        'stats': ['jensenshannon_dist', 'chi_squared_test', 'feature_corr', 'inv_kl_divergence',\n",
        "                 'max_mean_discrepancy', 'wasserstein_dist', 'survival_km_distance'],\n",
        "        'performance': ['linear_model', 'mlp', 'xgb']\n",
        "    }, use_cache=False)"
      ],
      "metadata": {
        "id": "MuFtdpE-Coe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "met_df"
      ],
      "metadata": {
        "id": "MW00l8IqMn5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualization\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "\n",
        "real_data=data.dropna()\n",
        "synthetic=samples.dropna()\n",
        "combined_data = pd.concat([real_data, synthetic], ignore_index=True)\n",
        "\n",
        "covariates = combined_data.drop(['duration', 'event'], axis=1)\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=0)\n",
        "tsne_result = tsne.fit_transform(covariates)\n",
        "event_type = combined_data['event']\n",
        "tsne_df = pd.DataFrame(data={'TSNE1': tsne_result[:, 0], 'TSNE2': tsne_result[:, 1], 'Event_Type': event_type})\n",
        "\n",
        "sns.scatterplot(x='TSNE1', y='TSNE2', hue='Event_Type', data=tsne_df[0:1904], palette='viridis')\n",
        "sns.scatterplot(x='TSNE1', y='TSNE2', hue='Event_Type', data=tsne_df[1904:])\n",
        "plt.title('t-SNE Plot of Covariates based on Event Type (Original and Synthetic)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UG5sfqZMCSIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data['x0'], bins=50, kde=False, label='Original Data', color='blue')\n",
        "sns.histplot(samples['x0'], bins=50, kde=False, label='Synthetic Data', color='orange')\n",
        "plt.title('Comparison of Covariate Distributions')\n",
        "plt.xlabel('Covariate Values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OOxKDw4ywxay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import BayesianGaussianMixture\n",
        "\n",
        "def fit_dpmm_and_sample(data, sample_size, integer_sampling=False, bandwidth='scott'):\n",
        "   \"\"\"\n",
        "   Fit DPMM to data and sample from it\n",
        "   Parameters:\n",
        "   -----------\n",
        "   data : array-like\n",
        "       Input data to fit DPMM\n",
        "   sample_size : int\n",
        "       Number of samples to generate\n",
        "   integer_sampling : bool, default=False\n",
        "       If True, returns integer samples. If False, returns continuous samples\n",
        "   bandwidth : str or float, default='scott'\n",
        "       Not used for DPMM but kept for API consistency\n",
        "   Returns:\n",
        "   --------\n",
        "   array-like\n",
        "       Sampled values from the fitted DPMM\n",
        "   \"\"\"\n",
        "   # Reshape data for DPMM\n",
        "   data = data.reshape(-1, 1)\n",
        "\n",
        "   # Fit DPMM\n",
        "   dpmm = BayesianGaussianMixture(\n",
        "       n_components=10,  # Max number of components\n",
        "       weight_concentration_prior=1.0,\n",
        "       random_state=42\n",
        "   )\n",
        "   dpmm.fit(data)\n",
        "\n",
        "   if integer_sampling:\n",
        "       # Sample more points than needed to account for rounding and filtering\n",
        "       oversampling_factor = 1.5\n",
        "       samples = dpmm.sample(int(sample_size * oversampling_factor))[0].reshape(-1)\n",
        "       # Round to nearest integer and ensure positive\n",
        "       samples = np.round(np.abs(samples))\n",
        "       # Convert to integers\n",
        "       samples = samples.astype(int)\n",
        "       # Remove any zeros\n",
        "       samples = samples[samples > 0]\n",
        "       # If we have more samples than needed due to oversampling, randomly select\n",
        "       if len(samples) > sample_size:\n",
        "           samples = np.random.choice(samples, size=sample_size, replace=False)\n",
        "       # If we have fewer samples than needed, resample with replacement\n",
        "       elif len(samples) < sample_size:\n",
        "           samples = np.random.choice(samples, size=sample_size, replace=True)\n",
        "   else:\n",
        "       # Direct sampling for continuous values\n",
        "       samples = dpmm.sample(sample_size)[0].reshape(-1)\n",
        "       # Ensure all samples are positive\n",
        "       samples = np.abs(samples)\n",
        "\n",
        "   return samples"
      ],
      "metadata": {
        "id": "_v_l8wg_zWqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample t and e using DPMM\n",
        "\n",
        "survival_df=df\n",
        "event_0_data = survival_df[survival_df['event'] == 0]['duration'].values\n",
        "event_1_data = survival_df[survival_df['event'] == 1]['duration'].values\n",
        "\n",
        "# Use KDE to sample time values for each event type\n",
        "sample_size_0 = len(event_0_data)\n",
        "sample_size_1 = len(event_1_data)\n",
        "\n",
        "# Sample from KDE for each event type\n",
        "sample_event_0 = fit_dpmm_and_sample(event_0_data, sample_size_0,integer_sampling=True)\n",
        "sample_event_1 = fit_dpmm_and_sample(event_1_data, sample_size_1,integer_sampling=True)\n",
        "\n",
        "z=np.concatenate([sample_event_0,sample_event_1])\n",
        "x=np.zeros(len(sample_event_0))\n",
        "y=np.ones(len(sample_event_1))\n",
        "p=np.concatenate([x,y])\n",
        "\n",
        "## uncomment below sample t and e empirically\n",
        "# survival_df=data\n",
        "# event_0_data = survival_df[survival_df['event'] == 0]['duration']\n",
        "# event_1_data = survival_df[survival_df['event'] == 1]['duration']\n",
        "\n",
        "# sample_size_0 = len(event_0_data)\n",
        "# sample_size_1= len(event_1_data)\n",
        "# sample_event_0 = np.random.choice(event_0_data, size=sample_size_0)\n",
        "# sample_event_1 = np.random.choice(event_1_data, size=sample_size_1)\n",
        "\n",
        "# z=np.concatenate([sample_event_0,sample_event_1])\n",
        "# x=np.zeros(len(sample_event_0))\n",
        "# y=np.ones(len(sample_event_1))\n",
        "# p=np.concatenate([x,y])"
      ],
      "metadata": {
        "id": "vTFyMYF9xKsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# new_df = data[['event', 'duration']]\n",
        "new_df = data[['event', 'duration','x0']]\n",
        "new_df= new_df.drop(['event','duration'],axis=1)\n",
        "new_df['duration'] = z\n",
        "new_df['event'] = p\n",
        "new_df=new_df.drop('x0',axis=1)\n",
        "\n",
        "# generate sentences for conditionining\n",
        "\n",
        "def dataframe_to_text_df(df):\n",
        "    text_data = []\n",
        "    for index, row in df.iterrows():\n",
        "        row_text = \"\"\n",
        "        for col_name, col_value in row.items():\n",
        "            row_text += f\"{col_name} is {col_value},\"\n",
        "        text_data.append(row_text[:-1])  # Remove the trailing comma\n",
        "    text_df = pd.DataFrame({'text': text_data})\n",
        "    return text_df, text_data\n",
        "\n",
        "resulting_text_df, str_list = dataframe_to_text_df(new_df)"
      ],
      "metadata": {
        "id": "w2LPbiASM2BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=great.great_sample(\n",
        "    starting_prompts=str_list,\n",
        "    # temperature: float = 0.7,\n",
        "    # max_length: int = 100,\n",
        "    # device: str = 'cuda'\n",
        ")"
      ],
      "metadata": {
        "id": "ifdJg5oKNt4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "5KvXJAc4QFD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(df[df.isin(['placeholder']).any(axis=1)].index)\n",
        "df = df.astype(data.dtypes)\n",
        "df['event'] = pd.to_numeric(df['event'], errors='coerce')\n",
        "df['event'] = df['event'].round().astype('Int64')\n",
        "diff=len(data)-len(df)\n",
        "drop_indices = data.sample(diff).index\n",
        "data = data.drop(drop_indices)"
      ],
      "metadata": {
        "id": "lFx3JFdmcUsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "exlvcCKLfssI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Benchmark generated conditional data\n",
        "\n",
        "loader1 = SurvivalAnalysisDataLoader(data, target_column=\"event\", time_to_event_column=\"duration\")\n",
        "loader2 = SurvivalAnalysisDataLoader(df, target_column=\"event\", time_to_event_column=\"duration\")\n",
        "\n",
        "met_df = Metrics.evaluate(X_gt=loader1, X_syn=loader2, task_type='survival_analysis', metrics={\n",
        "        'stats': ['jensenshannon_dist', 'chi_squared_test', 'feature_corr', 'inv_kl_divergence', 'ks_test',\n",
        "                 'max_mean_discrepancy', 'wasserstein_dist', 'prdc', 'alpha_precision', 'survival_km_distance'],\n",
        "        'performance': ['linear_model', 'mlp', 'xgb', 'feat_rank_distance']\n",
        "    }, use_cache=False)\n",
        "\n",
        "met_df"
      ],
      "metadata": {
        "id": "XX1eRxKhQTjj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}